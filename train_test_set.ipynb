{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************\n",
    "# train_set, test_set 훈련세트, 테스트 세트, 검증세트\n",
    "# **************\n",
    "# feature : 컬럼, 변수\n",
    "# epoch : 타임, 횟수\n",
    "\"\"\"\n",
    "https://tykimos.github.io/2017/03/25/Dataset_and_Fit_Talk/ 참고\n",
    "\n",
    "\n",
    "    얼마정도 반복을 해야 최고의 학습효과를 낼 수 있을것인가를 정하기 위해서 검증셋을 사용.\n",
    "    훈련셋을 몇번 반복해서 학습할 것인가를 정할떄 그 횟수를 epoch이라고 한다.\n",
    "\"\"\"\n",
    "\n",
    "# 지도학습\n",
    "\"\"\" 레이블 있음(답)\n",
    "트레이닝 데이터셋으로 학습이 끝나면, 레이블이 지정되지 않은 테스트 셋을 이용해서 학습된 알고리즘이 얼마나 정확히 예측하는지를 측정\n",
    "예) MNIST 데이터셋\n",
    "[알고리즙 종류]\n",
    "KNN\n",
    "결정트리\n",
    "랜덤포레스트,\n",
    "나이브 베이지안\n",
    "SVM\n",
    "[활용라이브러리] -sklearn\n",
    "\"\"\"\n",
    "\n",
    "# 비지도학습\n",
    "\"\"\" \n",
    "데이터에 대한 명시적인 정답(레이블)이 주어지지 않은 상태에서 컴퓨너를 학습시키는 방법론 - 클러스터링\n",
    "\n",
    "데이터의 숨겨진 특징(feature)를 찾거나 구조를 발견하는데 사용\n",
    "[알고리즘 종류]\n",
    "KMEANS(K평균)\n",
    "\"\"\"\n",
    "\n",
    "# 강화학습\n",
    "\"\"\"\n",
    "에이전트가 주어진환경(state)에 대해 어떤 행동(action)을 취하고 이로부터 어떤보상(reward)를 얻으면서 학습을 진행한다.\n",
    "이때, 에이전트느 보상을 최대화 하도록 학습한다.\n",
    "따라서 강화학습은 일종의 동적인 상태(dynamic envirnment)에서 데이터를 수집하는 과정까지 포함되어있다.\n",
    "[알고리즘 종류]\n",
    "Q-LEARNING 이 대표적이며 딥러닝과 결합하여\n",
    "Deep-Q-Network\n",
    "[활용라이브러리] -keras\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
