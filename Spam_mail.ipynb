{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import names\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: mcmullen gas for 11 / 99\n",
      "jackie ,\n",
      "since the inlet to 3 river plant is shut in on 10 / 19 / 99 ( the last day of\n",
      "flow ) :\n",
      "at what meter is the mcmullen gas being diverted to ?\n",
      "at what meter is hpl buying the residue gas ? ( this is the gas from teco ,\n",
      "vastar , vintage , tejones , and swift )\n",
      "i still see active deals at meter 3405 in path manager for teco , vastar ,\n",
      "vintage , tejones , and swift\n",
      "i also see gas scheduled in pops at meter 3404 and 3405 .\n",
      "please advice . we need to resolve this as soon as possible so settlement\n",
      "can send out payments .\n",
      "thanks\n",
      "Subject: stacey automated system generating 8 k per week parallelogram\n",
      "people are\n",
      "getting rich using this system ! now it ' s your\n",
      "turn !\n",
      "we ' ve\n",
      "cracked the code and will show you . . . .\n",
      "this is the\n",
      "only system that does everything for you , so you can make\n",
      "money\n",
      ". . . . . . . .\n",
      "because your\n",
      "success is . . . completely automated !\n",
      "let me show\n",
      "you how !\n",
      "click\n",
      "here\n",
      "to opt out click here % random _ text\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = \"C:/ezen_tensorflow/seoul_cctv/seoul_cctv/\"\n",
    "\n",
    "file_path = context +\"ham/0007.1999-12-14.farmer.ham.txt\"\n",
    "with open(file_path, 'r') as infile:  # r : open for reading 읽기전용열람\n",
    "    ham_sample = infile.read()\n",
    "print(ham_sample)\n",
    "\n",
    "file_path = context +\"spam/0058.2003-12-21.GP.spam.txt\"\n",
    "with open(file_path, 'r') as infile:  # r : open for reading 읽기전용열람\n",
    "    spam_sample = infile.read()\n",
    "print(spam_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words=\"english\", max_features=500)\n",
    "# stop_words 불용어 제거\n",
    "# max_features 는 가장 출현빈도가 높은 상위 500개의 단어만 고려함\n",
    "# 벡터화(Vectorizer) 는 문서의 행렬을 \"팀 도큐먼트 행렬\" 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails, labels = [], []\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = context + \"spam/\"\n",
    "for filename in glob.glob(os.path.join(file_path, '*.txt')):\n",
    "    with open(filename, 'r', encoding=\"ISO-8859-1\") as infile:  \n",
    "        emails.append(infile.read())\n",
    "        labels.append(1)  #스팸메일 : 1\n",
    "file_path = context + \"ham/\"\n",
    "for filename in glob.glob(os.path.join(file_path, '*.txt')):\n",
    "    with open(filename, 'r', encoding=\"ISO-8859-1\") as infile:  \n",
    "        emails.append(infile.read())\n",
    "        labels.append(0)   #정상메일 : 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letters_only(astr):\n",
    "    return astr.isalpha()   # 숫자와 구두점 표기 제거(알파벳만 남김)\n",
    "\n",
    "import nltk\n",
    "nltk.download('all') # 자연어 툴킷 전부 다운받음\n",
    "\n",
    "all_names = set(names.words())  # 사람이름제거(옵션)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(docs):\n",
    "    cleaned_docs = []\n",
    "    for doc in docs:\n",
    "        cleaned_docs.append(' '.join([lemmatizer.lemmatize(word.lower())  # 스페이스 \n",
    "                                                    for word in doc.split()\n",
    "                                                    if letters_only(word)\n",
    "                                                    and word not in all_names\n",
    "            ]) )\n",
    "    return cleaned_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 481)\t1\n",
      "  (0, 357)\t1\n",
      "  (0, 69)\t1\n",
      "  (0, 285)\t1\n",
      "  (0, 424)\t1\n",
      "  (0, 250)\t1\n",
      "  (0, 345)\t1\n",
      "  (0, 445)\t1\n",
      "  (0, 231)\t1\n",
      "  (0, 497)\t1\n",
      "  (0, 47)\t1\n",
      "  (0, 178)\t2\n",
      "  (0, 125)\t2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  (0, 481)\\t1\\n  (0, 357)\\t1\\n  (0, 69)\\t1\\n  (0, 285)\\t1\\n  (0, 424)\\t1\\n  (0, 250)\\t1\\n  (0, 345)\\t1\\n  (0, 445)\\t1\\n  (0, 231)\\t1\\n  (0, 497)\\t1\\n  (0, 47)\\t1\\n  (0, 178)\\t2\\n  (0, 125)\\t2\\n  희소벡터(sparse vector) : \"팀 도큐먼트 행렬\" 의 형태로 각 행이 문서와 메일의 용어 \\n  출현 빈도를 나타냄\\n  (row index, feature/term index) \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_emails = clean_text(emails)\n",
    "term_docs = cv.fit_transform(cleaned_emails)\n",
    "print(term_docs[0])\n",
    "\"\"\"  (0, 481)\t1\n",
    "  (0, 357)\t1\n",
    "  (0, 69)\t1\n",
    "  (0, 285)\t1\n",
    "  (0, 424)\t1\n",
    "  (0, 250)\t1\n",
    "  (0, 345)\t1\n",
    "  (0, 445)\t1\n",
    "  (0, 231)\t1\n",
    "  (0, 497)\t1\n",
    "  (0, 47)\t1\n",
    "  (0, 178)\t2\n",
    "  (0, 125)\t2\n",
    "  희소벡터(sparse vector) : \"팀 도큐먼트 행렬\" 의 형태로 각 행이 문서와 메일의 용어 \n",
    "  출현 빈도를 나타냄\n",
    "  (row index, feature/term index) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 용어 feature 를 key 로 , feature index(481) 을 value 로 어휘 사전 이용 가능\n",
    "feature_mapping = cv.vocabulary \n",
    "feature_names = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "website\n",
      "read\n",
      "energy\n"
     ]
    }
   ],
   "source": [
    "print(feature_names[481])  # website\n",
    "print(feature_names[357])  # read\n",
    "print(feature_names[125])  # energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'선행기반 훈련샘플\\nreturn 값은 dictionary\\nkey 값은 클래스 라벨\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사전확률(prior) 를 구하기 위해 레이블을 기준으로 데이터를 그룹화하기\n",
    "def get_label_index(labels):\n",
    "    from collections import defaultdict\n",
    "    label_index = defaultdict(list)\n",
    "    for index, label in enumerate(labels):\n",
    "        label_index[label].append(index)\n",
    "    return label_index\n",
    "\n",
    "\"\"\"선행기반 훈련샘플\n",
    "return 값은 dictionary\n",
    "key 값은 클래스 라벨\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'선행기반 likelihood 유사가능도\\nreturn 값은 dictionary\\nkey 값은 클래스 라벨\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사전확률(prior) 를 구하기\n",
    "def get_prior(label_index):\n",
    "    prior = {label: len(index) for label, index in label_index.items()} # { } 조심 !!\n",
    "    total_count = sum(prior.values())\n",
    "    for label in prior:\n",
    "        prior[label] /= float(total_count)\n",
    "    return prior\n",
    "\n",
    "\"\"\"선행기반 likelihood 유사가능도\n",
    "return 값은 dictionary\n",
    "key 값은 클래스 라벨\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'후행기반 테스트샘플\\nreturn 값은 dictionary\\nkey 값은 클래스 라벨\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_likelihood(term_document_matrix, label_index, smoothing=0):\n",
    "    likelihood = {}\n",
    "    for label, index in label_index.items():\n",
    "        likelihood[label] = term_document_matrix[index, :].sum(axis=0) + smoothing\n",
    "        likelihood[label] = np.asarray(likelihood[label])[0]\n",
    "        total_count = likelihood[label].sum() # 고쳐짐\n",
    "        total_count = likelihood[label] / float(total_count)\n",
    "    return likelihood\n",
    "\n",
    "feature_names[:5]\n",
    "\"\"\"['able', 'access', 'account', 'accounting', 'act']\"\"\"\n",
    "\"\"\"후행기반 테스트샘플\n",
    "return 값은 dictionary\n",
    "key 값은 클래스 라벨\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n리턴값은 딕셔너리 데이터 타입\\n키값은 클래스 레이블, 밸류는 관련한 사후확률값\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사후확률 계산하기\n",
    "\"\"\"\n",
    "리턴값은 딕셔너리 데이터 타입\n",
    "키값은 클래스 레이블, 밸류는 관련한 사후확률값\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior(term_document_matrix, prior, likelihood):\n",
    "    num_docs = term_document_matrix.shape[0]\n",
    "    posteriors =[]\n",
    "    for i in range(num_docs):\n",
    "        # 사후 확률은 사전 확률 * 유사 가능도에 비례\n",
    "        # = exp(log(사전확률 * 유사가능도))\n",
    "        # = exp(log(사전확률 + log(유사가능도))\n",
    "\n",
    "        posterior = {key: np.log(prior_label) for key, prior_label in prior.items()}  # 고침\n",
    "        for label, likelihood_label in likelihood.items():\n",
    "            term_document_vector = term_document_matrix.getrow(i)\n",
    "            counts = term_document_vector.data\n",
    "            indices = term_document_vector.indices\n",
    "            for count, index in zip(counts, indices):\n",
    "                posterior[label] += np.log(likelihood_label[index]) * count\n",
    "                # exp(-1000):exp(-999) 는 분모가 0이 되는 문제를 유발한다\n",
    "                # 하지만 이것은 exp(0):exp(1) 과 값이 같다\n",
    "            min_log_posterior = min(posterior.values())\n",
    "            for label in posterior:\n",
    "                try:\n",
    "                    posterior[label] = np.exp(posterior[label] - min_log_posterior)\n",
    "                except:\n",
    "                    # 어떤 값의 로그 치환값이 지나치게 클 경우\n",
    "                    # 이 값에는 무한대를 의미하는 'inf' 를 할당한다\n",
    "                    posterior[label] = float('inf')\n",
    "            # 전체 합이 1이 되도록 정규화 한다\n",
    "            sum_posterior = sum(posterior.values())\n",
    "            for label in posterior:\n",
    "                if posterior[label] == float('inf'):\n",
    "                    posterior[label] = 1.0\n",
    "                else:\n",
    "                    posterior[label] /= sum_posterior\n",
    "            posteriors.append(posterior.copy())\n",
    "        return posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = get_label_index(labels)\n",
    "prior = get_prior(label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'나이브 베이즈의 이러한 정규화 방법은 가짜 수(pseudocount) 1일 경우 \\n라플라스 정규화(Laplace smoothing)라고 불리고,\\n일반적으로 리드스톤 정규화(Lidstone smoothing) = 0라고 불린다.\\n높은 분류 성능을 얻고자 하면 1로 둔다.\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"나이브 베이즈의 이러한 정규화 방법은 가짜 수(pseudocount) 1일 경우 \n",
    "라플라스 정규화(Laplace smoothing)라고 불리고,\n",
    "일반적으로 리드스톤 정규화(Lidstone smoothing) = 0라고 불린다.\n",
    "높은 분류 성능을 얻고자 하면 1로 둔다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing = 1\n",
    "likelihood = get_likelihood(term_docs, label_index, smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_test = [\n",
    "    '''Subject: flat screens\n",
    "    hello ,\n",
    "    please call or contact regarding the other flat screens requested .\n",
    "    trisha tlapek - eb 3132 b\n",
    "    michael sergeev - eb 3132 a\n",
    "    also the sun blocker that was taken away from eb 3131 a .\n",
    "    trisha should two monitors also michael .\n",
    "    thanks\n",
    "    kevin moore''',\n",
    "    '''Subject: having problems in bed ? we can help !\n",
    "    cialis allows men to enjoy a fully normal sex life without having to plan the sexual act .\n",
    "    if we let things terrify us , life will not be worth living .\n",
    "    brevity is the soul of lingerie .\n",
    "    suspicion always haunts the guilty mind .'''\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{1: 0.9999999999593137, 0: 4.068630618433851e-11}, {1: 1.4314819702572765e-16, 0: 0.9999999999999999}]\n"
     ]
    }
   ],
   "source": [
    "cleanded_test = clean_text(email_test)\n",
    "term_docs_test = cv.transform(cleanded_test)\n",
    "posterior = get_posterior(term_docs_test, prior, likelihood)\n",
    "print(posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1: 0.9999999999593137, \\n0: 4.068630618433851e-11},  \\n첫번째 이메일은 99.5% 가 정상 메일\\n{1: 1.4314819702572765e-16, \\n0: 0.9999999999999999}]\\n두번째 이메일은 거의 100% 스팸메일로 나왔음\\n두 결과 모두 올바른 예측\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1: 0.9999999999593137, \n",
    "0: 4.068630618433851e-11},  \n",
    "첫번째 이메일은 99.5% 가 정상 메일\n",
    "{1: 1.4314819702572765e-16, \n",
    "0: 0.9999999999999999}]\n",
    "두번째 이메일은 거의 100% 스팸메일로 나왔음\n",
    "두 결과 모두 올바른 예측\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(cleaned_emails, labels, test_size=0.33, random_state = 42) # 테스트세트는 1/3, 랜덤값 시드 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1707, 1707)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(Y_train)\n",
    "len(X_test), len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_docs_train = cv.fit_transform(X_train)\n",
    "label_index = get_label_index(Y_train)\n",
    "prior = get_prior(label_index)\n",
    "likelihood = get_likelihood(term_docs_train, label_index, smoothing)\n",
    "\n",
    "term_docs_test = cv.transform(X_test)\n",
    "posterior = get_posterior(term_docs_test, prior, likelihood)\n",
    "\n",
    "correct = 0.0\n",
    "for pred, actual in zip(posterior, Y_test):\n",
    "    if actual ==1 :\n",
    "        if pred[1] >= 0.5:\n",
    "            correct += 1\n",
    "    elif pred[0] >= 0.5:\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확도 accuracy 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuray on 1707 testing samples is : 0.1%\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuray on {0} testing samples is : {1:.1f}%\".format(len(Y_test), correct/len(Y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB(alpha=1.0, fit_prior=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설정값이 1인 스무딩 파라미터를 사이킷 런에서는 alpha 라고 지칭함\n",
    "### 학습데이터세트를 가지고 학습된 사전확률 prior를 사이킷런에서는 fit_prior라 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(term_docs_train, Y_train) # fit 함수를 이용한 학습분류기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_prob = clf.predict_proba(term_docs_test) # 예측결과 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 2.12716600e-10],\n",
       "       [1.00000000e+00, 2.72887131e-75],\n",
       "       [6.34671963e-01, 3.65328037e-01],\n",
       "       [1.00000000e+00, 1.67181666e-12],\n",
       "       [1.00000000e+00, 4.15341124e-12],\n",
       "       [1.37860327e-04, 9.99862140e-01],\n",
       "       [0.00000000e+00, 1.00000000e+00],\n",
       "       [1.00000000e+00, 1.07066506e-18],\n",
       "       [1.00000000e+00, 2.02235745e-13],\n",
       "       [3.03193335e-01, 6.96806665e-01]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_prob[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5는 기본임계치, 만약 클래스의 예측확률이 0.5보다크면 클래스1에 할당됨. 그렇지 않으면 클래스 0에 할당됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = clf.predict(term_docs_test)\n",
    "prediction[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score 합수를 이용한 정확도 성능 측정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9203280609256005"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = clf.score(term_docs_test, Y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuray on testing samples is : 92.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuray on testing samples is : {0:.1f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
