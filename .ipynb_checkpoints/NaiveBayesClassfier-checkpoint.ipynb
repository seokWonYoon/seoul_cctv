{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pandas import read_table\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassfier:\n",
    "    def __init__(self, k=0.5):\n",
    "        self.k = k\n",
    "        self.word_probs = []\n",
    "    def load_corpus(self, path):\n",
    "        corpus = read_table(path, sep=',', encoding='UTF-8')\n",
    "        corpus = np.array(corpus)\n",
    "        return corpus\n",
    "    def count_words(self, training_set):\n",
    "        # 학습데이터는 영화리뷰 본문(doc), 평점(point) 로 구성\n",
    "        counts = defaultdict(lambda: [0,0])\n",
    "        for doc, point in training_set:\n",
    "            #영화리뷰가 text 일때만 카운트\n",
    "            if self.isNumber(doc) is False:\n",
    "                # 리뷰를 띄어쓰기 단위로 토크나이징\n",
    "                words = doc.split()\n",
    "                for word in words:\n",
    "                    counts[word][0 if point > 3.5 else 1] += 1\n",
    "        return counts\n",
    "    def isNumber(self, s):\n",
    "        try:\n",
    "            float(s)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    def word_probabilities(self, counts, total_class0, total_class1, k):\n",
    "        # 단어의 빈도수를 [단어, p(w|긍정), p(w|부정)] 형태로 변환\n",
    "        return [(w,\n",
    "                 (class0 + k) / (total_class0 + 2*k),\n",
    "                 (class1 + k) / (total_class1 + 2*k)) # (  ) 닫음\n",
    "                for w, (class0, class1) in counts.items()]\n",
    "    def class0_probability(self, word_probs, doc):\n",
    "        # 별도 토크나이즈 하지 않고 띄어쓰기로만\n",
    "        docwords = doc.split()\n",
    "        #초기값은 모두 0으로 처리\n",
    "        log_prob_if_class0 = log_prob_if_class1 = 0.0\n",
    "        # 모든 단어에 대한 반복\n",
    "        for word, prob_if_class0, prob_if_class1 in word_probs:\n",
    "            # 만약 리뷰에 word 가 나타나면 해당 단어가 나올 log 에 확률을 더해 줌\n",
    "            if word in docwords:\n",
    "                log_prob_if_class0 += math.log(prob_if_class0)\n",
    "                log_prob_if_class1 += math.log(prob_if_class1)\n",
    "            #만약 리뷰에 word 가 나타나지 않는다면 \n",
    "            # 해당 단어가 나오지 않을 log 에 확률을 더해줌\n",
    "            # 나오지 않을 확률은 log(1-나올확률)로 계산\n",
    "            else:\n",
    "                log_prob_if_class0 += math.log(1.0 - prob_if_class0)\n",
    "                log_prob_if_class1 += math.log(1.0 - prob_if_class1)\n",
    "        prob_if_class0 = math.exp(log_prob_if_class0)\n",
    "        prob_if_class1 = math.exp(log_prob_if_class1)\n",
    "        return prob_if_class0 / (prob_if_class0 + prob_if_class1)\n",
    "    def train(self, trainfile_path):\n",
    "        training_set =  self.load_corpus(trainfile_path)\n",
    "\n",
    "        # 범주0(긍정)과 범주1(부정) 문서의 수를 세어 줌\n",
    "        num_class0 = len([1 for _, point in training_set if point > 3.5])\n",
    "        num_class1 = len(training_set) - num_class0\n",
    "\n",
    "        # train\n",
    "        word_counts = self.count_words(training_set)\n",
    "        self.word_probs = self.word_probabilities(word_counts, num_class0, num_class1, self.k)\n",
    "    def classify(self, doc):\n",
    "        return self.class0_probability(self.word_probs, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
